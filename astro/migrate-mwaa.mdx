---
sidebar_label: "From MWAA"
title: "Migrate to Astro from MWAA"
id: migrate-mwaa
description: Get started on Astro by migrating your Airflow code from Amazon Managed Workflows for Apache Airflow (MWAA).
---

import Prerequisites from "./migration-partials/prereqs.mdx";
import StarshipUsage from "./migration-partials/starship-usage.mdx";
import Workspaces from "./migration-partials/workspaces.mdx";
import Deployments from "./migration-partials/deployments.mdx";
import DirectorySetup from "./migration-partials/directory-setup.mdx";
import FinalStep from "./migration-partials/final-step.mdx";
import Deploy from "./migration-partials/deploy.mdx";

This is where you'll find instructions for migrating an Airflow environment from [Amazon Managed Workflows for Apache Airflow (MWAA)](https://cloud.google.com/composer/docs/concepts/overview) to Astro.

To complete the migration process, you will:

- Prepare your source Airflow and set up your Astro Airflow environment.
- Migrate metadata from your source Airflow environment.
- Migrate DAGs and additional Airflow components from your source Airflow environment.
- Complete the cutover process to Astro.

## Prerequisites

<Prerequisites />

You can additionally use the [AWS CLI](https://aws.amazon.com/cli/) to expedite some steps in this guide.

## Step 1: Install Astronomer Starship

The [Astronomer Starship](https://pypi.org/project/astronomer-starship/) migration utility connects your source Airflow environment to your Astro Deployment and migrates your Airflow connections, Airflow variables, environment variables, and DAGs.

The Starship migration utility works as a [plugin](https://github.com/astronomer/starship/tree/master/astronomer-starship) with a user interface, or as an Airflow operator if you are migrating from a more restricted Airflow environment.

See the following table for information on which versions of Starship are available depending on your source Airflow environment:

| Source Airflow environment | Starship plugin | Starship operator |
| -------------------------- | --------------- | ----------------- |
| Airflow 1.x                | ❌              | ❌                |
| MWAA v2.0.2                |                 | ✔️️                 |
| MWAA v2.2.2                | ✔️️               |                   |
| MWAA v2.4.3                | ✔️️               |                   |

To install Starship to your MWAA instance as a plugin:

1. Download the `requirements.txt` file for your source Airflow environment from S3. See [AWS documentation](https://docs.aws.amazon.com/mwaa/latest/userguide/best-practices-dependencies.html#best-practices-dependencies-different-ways).
2. Add `astronomer-starship` on a new line to your `requirements.txt` file.
3. Reupload the file to your S3 bucket.
4. Update your Airflow environment to use the new version of this file.

:::tip

To complete this setup from the command line:

1. Run the following commands to set environment variables on your local machine:

    ```sh
    export MWAA_NAME="MWAA"
    export MWAA_BUCKET="MWAA BUCKET"
    ```

2. Run the following AWS CLI commands to install Starship:

    ```shell
    aws s3 cp "s3://$MWAA_BUCKET/requirements.txt" requirements.txt
    echo 'astronomer-starship' >> requirements.txt
    aws s3 cp requirements.txt "s3://$MWAA_BUCKET/requirements.txt"
    aws mwaa update-environment "$MWAA_NAME" --requirements-s3-object-version="$(aws s3api head-object --bucket=$MWAA_BUCKET --key=requirements.txt --query="VersionId")"
    ```

:::

## Step 2: Create an Astro Workspace

<Workspaces />

## Step 3: Create an Astro Deployment

<Deployments />

## Step 4: Use Starship to Migrate Airflow Connections and Variables

<StarshipUsage />

## Step 5: Create an Astro project

<DirectorySetup />

## Step 6: Migrate project code and dependencies to your Astro project

1. Open your Astro project Dockerfile. Update the Runtime version in first line to the version you selected for your Deployment in [Step 3](#step-3-create-astro-deployment). For example, if your Runtime version was 6.3.0, your Dockerfile would look like the following:

    ```Dockerfile
    FROM quay.io/astronomer/astro-runtime:6.3.0
    ```

    The `Dockerfile` defines the environment that all your Airflow components run in. You can modify it to make certain resources available to your Airflow environment like certificates or keys. For this migration, you only need to update your Runtime version. 

2. Open your Astro project `requirements.txt` file and add all Python packages from your source Airflow environment's `requirements.txt` file. See [AWS documentation](https://docs.aws.amazon.com/mwaa/latest/userguide/working-dags-dependencies.html) to find this file in your S3 bucket.

  :::tip 

  To directly copy your PyPI packages from AWS to your Astro project `requirements.txt` file, run the following AWS CLI command:

  ```sh
  aws s3 cp s3://[BUCKET]/requirements.txt requirements.txt
  ```  

  Review the output in `requirements.txt` after running this command to ensure that all packages were imported on their own line of text.    

  :::

3. Open your Astro project `dags` folder. Add your DAG files from either your source control platform or S3. 

  :::tip

  To directly copy your PyPI packages from AWS to your Astro project `requirements.txt` file, run the following AWS CLI command:
  
  ```sh
  aws s3 cp --recursive s3://[BUCKET]/dags dags
  ```  
  
  Review your DAG files in `dags` after running this command to ensure that all DAGs were successfully exported. 

  :::

4. If you utilized the [`plugins` folder](https://cloud.google.com/composer/docs/concepts/cloud-storage#folders_in_the_bucket) in your MWAA project, copy the contents of this folder from your source control platform or S3 to your Astro project `/plugins` folder.

  :::tip
  
  To directly copy your PyPI packages from AWS to your Astro project `requirements.txt` file, run the following AWS CLI command:
  
  ```sh
  aws s3 cp --recursive s3://[BUCKET]/plugins.zip plugins.zip
  unzip plugins.zip
  ```
  
  Review the contents of your Astro project `plugins` folder after running this command to ensure that all files were successfully exported. 
  
  :::

## Step 7: Configure additional data pipeline infrastructure

The core migration of your project is now complete. Read the following topics to see whether you need to set up any additional infrastructure on Astro before cutting over your DAGs. 

### Set up CI/CD

If you used CI/CD to deploy code to your source Airflow environment, read the following documentation to learn about setting up a similar CI/CD pipeline for your Astro project:

- [Set-Up CI/CD](set-up-ci-cd.md)
- [CI/CD templates](ci-cd.md)

### Set up a secrets backend

If you currently store Airflow variables or connections in a secrets backend, you need to integrate your secrets backend with Astro to access those objects from your migrated DAGs. See [Configure a Secrets Backend](secrets-backend.md) for setup steps.

## Step 8: Test locally and check for import errors

Depending on how thoroughly you want to test your Airflow environment, you have a few options for testing your project locally before deploying to Astro.

- In your Astro project directory, run `astro dev parse` to check for any parsing errors in your DAGs.
- Run `astro run <dag-id>` to test a specific DAG. This command compiles your DAG and runs it in a single Airflow worker container based on your Astro project configurations.
- Run `astro dev start` to start a complete Airflow environment on your local machine. After your project starts up, you can access the Airflow UI at `localhost:8080`. See [Test and troubleshoot locally](test-and-troubleshoot-locally.md).

Note that your migrated Airflow variables and connections are not available locally. You must deploy your project to Astro to test these resources. 

## Step 9: Deploy to Astro

<Deploy />

## Step 10: Cut over from your source Airflow environment to Astro

<FinalStep />
